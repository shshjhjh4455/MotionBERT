{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한 문장으로 정의:\n",
    "입력1: 한 사람의 2D영상(.mp4) - youtube에서 다운로드\n",
    "\n",
    "출력1: 2D영상의 keypoint(.json, mp4) - Using AlphaPose\n",
    " https://github.com/shshjhjh4455/MotionBERT/blob/main/sample/AlphaPose_4.mp4, https://github.com/shshjhjh4455/MotionBERT/blob/main/sample/alphapose-results.json\n",
    "\n",
    "입력2: 출력1\n",
    "\n",
    "출력2: 3D영상([3D Pose](https://github.com/shshjhjh4455/MotionBERT/blob/main/infer_wild.py), [Mesh]((https://github.com/shshjhjh4455/MotionBERT/blob/main/infer_wild_mesh.py))) (.mp4) - https://github.com/shshjhjh4455/MotionBERT/tree/main/sample/output 출력소요시간 약40분 using colab+ A100 GPU \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해당 프로젝트의 핵심 알고리즘 -> DSTfromer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 소개:\n",
    "-DSTformer은 3D 자세 추정과 동작 인식 분야에서 중요한 역할을 하는 딥러닝 모델입니다.\n",
    "\n",
    "-자세 추정과 동작 인식은 인간의 자세와 동작을 정확하게 추정하고 분류하는 작업으로, 로봇 제어, 운동 분석, 가상 현실 등 다양한 응용 분야에서 활용됩니다.\n",
    "\n",
    "-DSTformer은 시간(Temporal)과 공간(Spatial) 정보를 효과적으로 모델링하여 입력 데이터의 시공간적인 패턴을 학습하고, 시간과 공간의 상호작용을 포착할 수 있는 딥러닝 기반 알고리즘입니다.\n",
    "\n",
    "-이를 통해 DSTformer는 정확한 3D 자세 추정과 동작 인식을 수행할 수 있으며, 이는 인간의 움직임을 이해하고 상황에 맞는 응답을 제공하는 다양한 응용 분야에서 큰 가치를 가지게 됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알고리즘 개요:\n",
    "[Feature1](https://github.com/shshjhjh4455/MotionBERT/blob/main/img/Screenshot%202023-06-13%20at%2012.29.05%20PM.png)\n",
    "\n",
    "[Feature2](https://github.com/shshjhjh4455/MotionBERT/blob/main/img/Screenshot%202023-06-13%20at%2012.29.24%20PM.png)\n",
    "\n",
    "DSTformer 알고리즘의 핵심 아이디어를 간략히 설명합니다.\n",
    "\n",
    "시간과 공간 정보를 효과적으로 모델링하기 위해 Dual Spatial-Temporal Transformer를 사용한다는 점을 강조합니다.\n",
    "\n",
    "예시: \"DSTformer 알고리즘은 시간(Temporal)과 공간(Spatial) 정보를 효과적으로 모델링하기 위해 Dual Spatial-Temporal Transformer를 사용합니다. 이를 통해 DSTformer는 입력 데이터의 시간적 및 공간적인 패턴을 모델링하고, 시간과 공간의 상호작용을 포착할 수 있습니다.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 발표는 해당 프로젝트의 핵심 알고리즘인 DSTformer에 대해서 설명합니다. DSTformer.py는 [learning.py](https://github.com/shshjhjh4455/MotionBERT/blob/main/lib/utils/learning.py)에서 사용되며, 이는 [학습](https://github.com/shshjhjh4455/MotionBERT/blob/main/docs/action.md) 시 사용됩니다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 구조:\n",
    "DSTformer의 전체적인 네트워크 구조를 설명합니다.\n",
    "\n",
    "MLP, Attention, Block 등의 구성 요소에 대한 역할과 동작 방식을 설명합니다.\n",
    "\n",
    "예시: \"DSTformer는 MLP, Attention, Block 등의 구성 요소로 이루어진 네트워크 구조로 구성됩니다. MLP는 다층 퍼셉트론으로, 입력 데이터에 대한 선형 변환과 활성화 함수(GELU)를 적용하고 일부 뉴런을 무작위로 비활성화(Dropout)합니다. Attention은 입력 데이터의 시간적 및 공간적인 상호작용을 계산하고 중요한 정보에 가중치를 부여합니다. Block은 여러 개의 MLP와 Attention 레이어로 구성되어 시간 및 공간적 관계를 모델링합니다.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '원본 코드 설명' + '수정 및 개선한 부분'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본코드\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from lib.model.drop import DropPath\n",
    "\n",
    "def _no_grad_trunc_normal_(tensor, mean, std, a, b):\n",
    "    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n",
    "    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
    "    def norm_cdf(x):\n",
    "        # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        warnings.warn(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n",
    "                      \"The distribution of values may be incorrect.\",\n",
    "                      stacklevel=2)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Values are generated by using a truncated uniform distribution and\n",
    "        # then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to\n",
    "        # [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated\n",
    "        # standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):\n",
    "    # type: (Tensor, float, float, float, float) -> Tensor\n",
    "    r\"\"\"Fills the input Tensor with values drawn from a truncated\n",
    "    normal distribution. The values are effectively drawn from the\n",
    "    normal distribution :math:`\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n",
    "    with values outside :math:`[a, b]` redrawn until they are within\n",
    "    the bounds. The method used for generating the random values works\n",
    "    best when :math:`a \\leq \\text{mean} \\leq b`.\n",
    "    Args:\n",
    "        tensor: an n-dimensional `torch.Tensor`\n",
    "        mean: the mean of the normal distribution\n",
    "        std: the standard deviation of the normal distribution\n",
    "        a: the minimum cutoff value\n",
    "        b: the maximum cutoff value\n",
    "    Examples:\n",
    "        >>> w = torch.empty(3, 5)\n",
    "        >>> nn.init.trunc_normal_(w)\n",
    "    \"\"\"\n",
    "    return _no_grad_trunc_normal_(tensor, mean, std, a, b)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., st_mode='vanilla'):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.mode = st_mode\n",
    "        if self.mode == 'parallel':\n",
    "            self.ts_attn = nn.Linear(dim*2, dim*2)\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.attn_count_s = None\n",
    "        self.attn_count_t = None\n",
    "\n",
    "    def forward(self, x, seqlen=1):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        if self.mode == 'series':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'parallel':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x_t = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "            x_s = self.forward_spatial(q, k, v)\n",
    "            \n",
    "            alpha = torch.cat([x_s, x_t], dim=-1)\n",
    "            alpha = alpha.mean(dim=1, keepdim=True)\n",
    "            alpha = self.ts_attn(alpha).reshape(B, 1, C, 2)\n",
    "            alpha = alpha.softmax(dim=-1)\n",
    "            x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]\n",
    "        elif self.mode == 'coupling':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_coupling(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'vanilla':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "        elif self.mode == 'temporal':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'spatial':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "        else:\n",
    "            raise NotImplementedError(self.mode)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    def reshape_T(self, x, seqlen=1, inverse=False):\n",
    "        if not inverse:\n",
    "            N, C = x.shape[-2:]\n",
    "            x = x.reshape(-1, seqlen, self.num_heads, N, C).transpose(1,2)\n",
    "            x = x.reshape(-1, self.num_heads, seqlen*N, C) #(B, H, TN, c)\n",
    "        else:\n",
    "            TN, C = x.shape[-2:]\n",
    "            x = x.reshape(-1, self.num_heads, seqlen, TN // seqlen, C).transpose(1,2)\n",
    "            x = x.reshape(-1, self.num_heads, TN // seqlen, C) #(BT, H, N, C)\n",
    "        return x \n",
    "\n",
    "    def forward_coupling(self, q, k, v, seqlen=8):\n",
    "        BT, _, N, C = q.shape\n",
    "        q = self.reshape_T(q, seqlen)\n",
    "        k = self.reshape_T(k, seqlen)\n",
    "        v = self.reshape_T(v, seqlen)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = self.reshape_T(x, seqlen, inverse=True)\n",
    "        x = x.transpose(1,2).reshape(BT, N, C*self.num_heads)\n",
    "        return x\n",
    "\n",
    "    def forward_spatial(self, q, k, v):\n",
    "        B, _, N, C = q.shape\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = x.transpose(1,2).reshape(B, N, C*self.num_heads)\n",
    "        return x\n",
    "        \n",
    "    def forward_temporal(self, q, k, v, seqlen=8):\n",
    "        B, _, N, C = q.shape\n",
    "        qt = q.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "        kt = k.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "        vt = v.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "\n",
    "        attn = (qt @ kt.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ vt #(B, H, N, T, C)\n",
    "        x = x.permute(0, 3, 2, 1, 4).reshape(B, N, C*self.num_heads)\n",
    "        return x\n",
    "\n",
    "    def count_attn(self, attn):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "        attn = attn.mean(axis=1)\n",
    "        attn_t = attn[:, :, 1].mean(axis=1)\n",
    "        attn_s = attn[:, :, 0].mean(axis=1)\n",
    "        if self.attn_count_s is None:\n",
    "            self.attn_count_s = attn_s\n",
    "            self.attn_count_t = attn_t\n",
    "        else:\n",
    "            self.attn_count_s = np.concatenate([self.attn_count_s, attn_s], axis=0)\n",
    "            self.attn_count_t = np.concatenate([self.attn_count_t, attn_t], axis=0)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., mlp_out_ratio=1., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, st_mode='stage_st', att_fuse=False):\n",
    "        super().__init__()\n",
    "        # assert 'stage' in st_mode\n",
    "        self.st_mode = st_mode\n",
    "        self.norm1_s = norm_layer(dim)\n",
    "        self.norm1_t = norm_layer(dim)\n",
    "        self.attn_s = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode=\"spatial\")\n",
    "        self.attn_t = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode=\"temporal\")\n",
    "        \n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2_s = norm_layer(dim)\n",
    "        self.norm2_t = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        mlp_out_dim = int(dim * mlp_out_ratio)\n",
    "        self.mlp_s = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)\n",
    "        self.mlp_t = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)\n",
    "        self.att_fuse = att_fuse\n",
    "        if self.att_fuse:\n",
    "            self.ts_attn = nn.Linear(dim*2, dim*2)\n",
    "    def forward(self, x, seqlen=1):\n",
    "        if self.st_mode=='stage_st':\n",
    "            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))\n",
    "            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))\n",
    "        elif self.st_mode=='stage_ts':\n",
    "            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))\n",
    "            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))\n",
    "        elif self.st_mode=='stage_para':\n",
    "            x_t = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x_t = x_t + self.drop_path(self.mlp_t(self.norm2_t(x_t)))\n",
    "            x_s = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x_s = x_s + self.drop_path(self.mlp_s(self.norm2_s(x_s)))\n",
    "            if self.att_fuse:\n",
    "                #             x_s, x_t: [BF, J, dim]\n",
    "                alpha = torch.cat([x_s, x_t], dim=-1)\n",
    "                BF, J = alpha.shape[:2]\n",
    "                # alpha = alpha.mean(dim=1, keepdim=True)\n",
    "                alpha = self.ts_attn(alpha).reshape(BF, J, -1, 2)\n",
    "                alpha = alpha.softmax(dim=-1)\n",
    "                x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]\n",
    "            else:\n",
    "                x = (x_s + x_t)*0.5\n",
    "        else:\n",
    "            raise NotImplementedError(self.st_mode)\n",
    "        return x\n",
    "    \n",
    "class DSTformer(nn.Module):\n",
    "    def __init__(self, dim_in=3, dim_out=3, dim_feat=256, dim_rep=512,\n",
    "                 depth=5, num_heads=8, mlp_ratio=4, \n",
    "                 num_joints=17, maxlen=243, \n",
    "                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, att_fuse=True):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_feat = dim_feat\n",
    "        self.joints_embed = nn.Linear(dim_in, dim_feat)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks_st = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_st\")\n",
    "            for i in range(depth)])\n",
    "        self.blocks_ts = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_ts\")\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(dim_feat)\n",
    "        if dim_rep:\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(dim_feat, dim_rep)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "        self.head = nn.Linear(dim_rep, dim_out) if dim_out > 0 else nn.Identity()            \n",
    "        self.temp_embed = nn.Parameter(torch.zeros(1, maxlen, 1, dim_feat))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat))\n",
    "        trunc_normal_(self.temp_embed, std=.02)\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.att_fuse = att_fuse\n",
    "        if self.att_fuse:\n",
    "            self.ts_attn = nn.ModuleList([nn.Linear(dim_feat*2, 2) for i in range(depth)])\n",
    "            for i in range(depth):\n",
    "                self.ts_attn[i].weight.data.fill_(0)\n",
    "                self.ts_attn[i].bias.data.fill_(0.5)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, dim_out, global_pool=''):\n",
    "        self.dim_out = dim_out\n",
    "        self.head = nn.Linear(self.dim_feat, dim_out) if dim_out > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, return_rep=False):   \n",
    "        B, F, J, C = x.shape\n",
    "        x = x.reshape(-1, J, C)\n",
    "        BF = x.shape[0]\n",
    "        x = self.joints_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        _, J, C = x.shape\n",
    "        x = x.reshape(-1, F, J, C) + self.temp_embed[:,:F,:,:]\n",
    "        x = x.reshape(BF, J, C)\n",
    "        x = self.pos_drop(x)\n",
    "        alphas = []\n",
    "        for idx, (blk_st, blk_ts) in enumerate(zip(self.blocks_st, self.blocks_ts)):\n",
    "            x_st = blk_st(x, F)\n",
    "            x_ts = blk_ts(x, F)\n",
    "            if self.att_fuse:\n",
    "                att = self.ts_attn[idx]\n",
    "                alpha = torch.cat([x_st, x_ts], dim=-1)\n",
    "                BF, J = alpha.shape[:2]\n",
    "                alpha = att(alpha)\n",
    "                alpha = alpha.softmax(dim=-1)\n",
    "                x = x_st * alpha[:,:,0:1] + x_ts * alpha[:,:,1:2]\n",
    "            else:\n",
    "                x = (x_st + x_ts)*0.5\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, F, J, -1)\n",
    "        x = self.pre_logits(x)         # [B, F, J, dim_feat]\n",
    "        if return_rep:\n",
    "            return x\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        return self.forward(x, return_rep=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 알고리즘 설명"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP:\n",
    "MLP의 역할과 구조를 설명합니다.\n",
    "\n",
    "입력 데이터에 대한 선형 변환과 활성화 함수(GELU)를 적용하고, 일부 뉴런을 무작위로 비활성화(Dropout)합니다.\n",
    "\n",
    "예시: \"MLP는 입력 데이터에 대한 선형 변환과 활성화 함수(GELU)를 적용하는 다층 퍼셉트론입니다. 이를 통해 입력 데이터의 비선형성을 모델링하고 일부 뉴런을 무작위로 비활성화하여 과적합을 방지합니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          Input\n",
    "            │\n",
    "       ┌────▼────┐\n",
    "  fc1 │          │\n",
    "       ▼          ▼\n",
    "   ┌────────┐  ┌────────┐\n",
    "   │ Linear │  │ Linear │\n",
    "   └────────┘  └────────┘\n",
    "       │          │\n",
    "    act()        act()\n",
    "       │          │\n",
    "   ┌────────┐  ┌────────┐\n",
    "   │ Dropout│  │ Dropout│\n",
    "   └────────┘  └────────┘\n",
    "       │          │\n",
    "   ┌────────┐  ┌────────┐\n",
    "   │ Linear │  │ Linear │\n",
    "   └────────┘  └────────┘\n",
    "       │          │\n",
    "      Output\n",
    "\n",
    "위의 표현은 MLP 클래스의 구성 요소와 데이터 흐름을 나타내며, 순차적으로 진행됩니다.\n",
    "\n",
    "입력 데이터가 fc1으로 들어가고, Linear 변환을 수행합니다.\n",
    "act() 함수를 통해 활성화 함수가 적용됩니다.\n",
    "Dropout을 통해 일부 뉴런이 무작위로 비활성화됩니다.\n",
    "다시 Linear 변환을 수행하고, 다시 Dropout이 적용됩니다.\n",
    "최종적으로 출력 데이터가 생성됩니다.\n",
    "이를 통해 MLP 클래스는 입력 데이터를 변환하고 비선형성을 모델링하는 다층 퍼셉트론 구조를 나타냅니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention:\n",
    "Attention의 역할과 동작 방식을 설명합니다.\n",
    "\n",
    "입력 데이터의 시간적 및 공간적인 상호작용을 계산하고, 중요한 정보에 가중치를 부여합니다.\n",
    "\n",
    "예시: \"Attention은 입력 데이터의 시간적 및 공간적인 상호작용을 계산하는 메커니즘입니다. 입력 데이터의 특성을 기반으로 유사도를 계산하고, 중요한 정보에 대한 가중치를 부여합니다. 이를 통해 모델은 입력 데이터에서 핵심적인 패턴을 추출할 수 있습니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., st_mode='vanilla'):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.mode = st_mode\n",
    "        if self.mode == 'parallel':\n",
    "            self.ts_attn = nn.Linear(dim*2, dim*2)\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.attn_count_s = None\n",
    "        self.attn_count_t = None\n",
    "\n",
    "    def forward(self, x, seqlen=1):\n",
    "        B, N, C = x.shape\n",
    "        \n",
    "        if self.mode == 'series':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'parallel':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x_t = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "            x_s = self.forward_spatial(q, k, v)\n",
    "            \n",
    "            alpha = torch.cat([x_s, x_t], dim=-1)\n",
    "            alpha = alpha.mean(dim=1, keepdim=True)\n",
    "            alpha = self.ts_attn(alpha).reshape(B, 1, C, 2)\n",
    "            alpha = alpha.softmax(dim=-1)\n",
    "            x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]\n",
    "        elif self.mode == 'coupling':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_coupling(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'vanilla':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "        elif self.mode == 'temporal':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_temporal(q, k, v, seqlen=seqlen)\n",
    "        elif self.mode == 'spatial':\n",
    "            qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "            q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "            x = self.forward_spatial(q, k, v)\n",
    "        else:\n",
    "            raise NotImplementedError(self.mode)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "    \n",
    "    def reshape_T(self, x, seqlen=1, inverse=False):\n",
    "        if not inverse:\n",
    "            N, C = x.shape[-2:]\n",
    "            x = x.reshape(-1, seqlen, self.num_heads, N, C).transpose(1,2)\n",
    "            x = x.reshape(-1, self.num_heads, seqlen*N, C) #(B, H, TN, c)\n",
    "        else:\n",
    "            TN, C = x.shape[-2:]\n",
    "            x = x.reshape(-1, self.num_heads, seqlen, TN // seqlen, C).transpose(1,2)\n",
    "            x = x.reshape(-1, self.num_heads, TN // seqlen, C) #(BT, H, N, C)\n",
    "        return x \n",
    "\n",
    "    def forward_coupling(self, q, k, v, seqlen=8):\n",
    "        BT, _, N, C = q.shape\n",
    "        q = self.reshape_T(q, seqlen)\n",
    "        k = self.reshape_T(k, seqlen)\n",
    "        v = self.reshape_T(v, seqlen)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = self.reshape_T(x, seqlen, inverse=True)\n",
    "        x = x.transpose(1,2).reshape(BT, N, C*self.num_heads)\n",
    "        return x\n",
    "\n",
    "    def forward_spatial(self, q, k, v):\n",
    "        B, _, N, C = q.shape\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v\n",
    "        x = x.transpose(1,2).reshape(B, N, C*self.num_heads)\n",
    "        return x\n",
    "        \n",
    "    def forward_temporal(self, q, k, v, seqlen=8):\n",
    "        B, _, N, C = q.shape\n",
    "        qt = q.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "        kt = k.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "        vt = v.reshape(-1, seqlen, self.num_heads, N, C).permute(0, 2, 3, 1, 4) #(B, H, N, T, C)\n",
    "\n",
    "        attn = (qt @ kt.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ vt #(B, H, N, T, C)\n",
    "        x = x.permute(0, 3, 2, 1, 4).reshape(B, N, C*self.num_heads)\n",
    "        return x\n",
    "\n",
    "    def count_attn(self, attn):\n",
    "        attn = attn.detach().cpu().numpy()\n",
    "        attn = attn.mean(axis=1)\n",
    "        attn_t = attn[:, :, 1].mean(axis=1)\n",
    "        attn_s = attn[:, :, 0].mean(axis=1)\n",
    "        if self.attn_count_s is None:\n",
    "            self.attn_count_s = attn_s\n",
    "            self.attn_count_t = attn_t\n",
    "        else:\n",
    "            self.attn_count_s = np.concatenate([self.attn_count_s, attn_s], axis=0)\n",
    "            self.attn_count_t = np.concatenate([self.attn_count_t, attn_t], axis=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input\n",
    "│\n",
    "┌──────▼───────┐\n",
    "  qkv(x)    seqlen\n",
    "└──────▼───────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    " reshape_T(q) │\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    " reshape_T(k) │\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    " reshape_T(v) │\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "   q @ k.transpose(-2, -1)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "      softmax(dim=-1)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "     attn_drop\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "   attn @ v\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "reshape_T(x, inverse=True)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "transpose(1,2) │\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "reshape(BT, N, C*num_heads)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "    transpose(0, 3, 2, 1, 4)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "reshape(B, N, C*num_heads)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "     proj(x)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    ┌────▼────┐\n",
    "   proj_drop(x)\n",
    "    └────▼────┘\n",
    "        │\n",
    "    Output\n",
    "\n",
    "위의 표현은 Attention 클래스의 구성 요소와 데이터 흐름을 나타내며, 순차적으로 진행됩니다.\n",
    "\n",
    "입력 데이터 x와 seqlen이 주어집니다.\n",
    "qkv(x)는 입력 데이터에 대한 Query, Key, Value를 계산합니다.\n",
    "reshape_T 함수를 통해 q, k, v를 필요한 형태로 재구성합니다.\n",
    "q와 k의 내적을 계산하고, 스케일링을 수행합니다.\n",
    "소프트맥스 함수를 통해 어텐션 가중치를 계산합니다.\n",
    "어텐션 드롭아웃을 적용합니다.\n",
    "어텐션 가중치와 v의 내적을 계산합니다.\n",
    "다시 x를 필요한 형태로 재구성합니다.\n",
    "proj를 통해 선형 변환을 수행합니다.\n",
    "proj_drop을 적용하여 드롭아웃을 적용합니다.\n",
    "최종 출력을 생성합니다.\n",
    "이를 통해 Attention 클래스는 입력 데이터의 어텐션 메커니즘을 활용하여 변환을 수행하는 모듈을 나타냅니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block:\n",
    "Block의 역할과 동작 방식을 설명합니다.\n",
    "\n",
    "여러 개의 MLP와 Attention 레이어로 구성되며, 시간 및 공간적 관계를 모델링합니다.\n",
    "\n",
    "예시: \"Block은 여러 개의 MLP와 Attention 레이어로 구성되어 시간 및 공간적 관계를 모델링합니다. Block은 재귀적으로 중첩되어 입력 데이터의 시공간적 관계를 계층적으로 모델링하고, 입력 데이터의 표현을 업데이트합니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., mlp_out_ratio=1., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, st_mode='stage_st', att_fuse=False):\n",
    "        super().__init__()\n",
    "        # assert 'stage' in st_mode\n",
    "        self.st_mode = st_mode\n",
    "        self.norm1_s = norm_layer(dim)\n",
    "        self.norm1_t = norm_layer(dim)\n",
    "        self.attn_s = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode=\"spatial\")\n",
    "        self.attn_t = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop, st_mode=\"temporal\")\n",
    "        \n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2_s = norm_layer(dim)\n",
    "        self.norm2_t = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        mlp_out_dim = int(dim * mlp_out_ratio)\n",
    "        self.mlp_s = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)\n",
    "        self.mlp_t = MLP(in_features=dim, hidden_features=mlp_hidden_dim, out_features=mlp_out_dim, act_layer=act_layer, drop=drop)\n",
    "        self.att_fuse = att_fuse\n",
    "        if self.att_fuse:\n",
    "            self.ts_attn = nn.Linear(dim*2, dim*2)\n",
    "    def forward(self, x, seqlen=1):\n",
    "        if self.st_mode=='stage_st':\n",
    "            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))\n",
    "            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))\n",
    "        elif self.st_mode=='stage_ts':\n",
    "            x = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_t(self.norm2_t(x)))\n",
    "            x = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x = x + self.drop_path(self.mlp_s(self.norm2_s(x)))\n",
    "        elif self.st_mode=='stage_para':\n",
    "            x_t = x + self.drop_path(self.attn_t(self.norm1_t(x), seqlen))\n",
    "            x_t = x_t + self.drop_path(self.mlp_t(self.norm2_t(x_t)))\n",
    "            x_s = x + self.drop_path(self.attn_s(self.norm1_s(x), seqlen))\n",
    "            x_s = x_s + self.drop_path(self.mlp_s(self.norm2_s(x_s)))\n",
    "            if self.att_fuse:\n",
    "                #             x_s, x_t: [BF, J, dim]\n",
    "                alpha = torch.cat([x_s, x_t], dim=-1)\n",
    "                BF, J = alpha.shape[:2]\n",
    "                # alpha = alpha.mean(dim=1, keepdim=True)\n",
    "                alpha = self.ts_attn(alpha).reshape(BF, J, -1, 2)\n",
    "                alpha = alpha.softmax(dim=-1)\n",
    "                x = x_t * alpha[:,:,:,1] + x_s * alpha[:,:,:,0]\n",
    "            else:\n",
    "                x = (x_s + x_t)*0.5\n",
    "        else:\n",
    "            raise NotImplementedError(self.st_mode)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block\n",
    "├── norm1_s: LayerNorm\n",
    "├── norm1_t: LayerNorm\n",
    "├── attn_s: Attention (st_mode=\"spatial\")\n",
    "├── attn_t: Attention (st_mode=\"temporal\")\n",
    "├── drop_path: DropPath\n",
    "├── norm2_s: LayerNorm\n",
    "├── norm2_t: LayerNorm\n",
    "├── mlp_s: MLP\n",
    "├── mlp_t: MLP\n",
    "└── ts_attn: Linear (if att_fuse=True)\n",
    "\n",
    "위의 구조는 Block 클래스의 구성 요소를 나타냅니다. Block은 DSTformer의 핵심 블록으로 사용되며, 다양한 유형의 어텐션과 MLP 레이어를 포함합니다.\n",
    "\n",
    "norm1_s와 norm1_t: 입력 데이터의 Layer Normalization\n",
    "attn_s: 시공간 어텐션 (Spatial Attention)\n",
    "attn_t: 시계열 어텐션 (Temporal Attention)\n",
    "drop_path: 스토캐스틱 드롭 패스 (Stochastic Drop Path)\n",
    "norm2_s와 norm2_t: 어텐션 이후의 Layer Normalization\n",
    "mlp_s: 시공간 MLP\n",
    "mlp_t: 시계열 MLP\n",
    "ts_attn (if att_fuse=True): 시간과 공간 어텐션을 결합하는 선형 레이어\n",
    "위의 시각적 표현은 코드에서 각 구성 요소의 계층 구조와 관련 함수를 보여줍니다. 이를 통해 Block 클래스의 구성과 각 구성 요소의 역할을 이해할 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Spatial-Temporal Transformer:\n",
    "시간과 공간 정보를 모델링하기 위해 Dual Spatial-Temporal Transformer를 사용한다는 개념을 강조합니다.\n",
    "\n",
    "시간과 공간 정보의 상호작용을 효과적으로 포착할 수 있는 구조를 설명합니다.\n",
    "\n",
    "예시: \"DSTformer는 시간(Temporal)과 공간(Spatial) 정보를 모델링하기 위해 Dual Spatial-Temporal Transformer를 사용합니다. 이 구조는 시간과 공간 정보의 상호작용을 효과적으로 포착할 수 있는 기능을 제공합니다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DSTformer(nn.Module):\n",
    "    def __init__(self, dim_in=3, dim_out=3, dim_feat=256, dim_rep=512,\n",
    "                 depth=5, num_heads=8, mlp_ratio=4, \n",
    "                 num_joints=17, maxlen=243, \n",
    "                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, att_fuse=True):\n",
    "        super().__init__()\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_feat = dim_feat\n",
    "        self.joints_embed = nn.Linear(dim_in, dim_feat)\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks_st = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_st\")\n",
    "            for i in range(depth)])\n",
    "        self.blocks_ts = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_ts\")\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(dim_feat)\n",
    "        if dim_rep:\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(dim_feat, dim_rep)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "        self.head = nn.Linear(dim_rep, dim_out) if dim_out > 0 else nn.Identity()            \n",
    "        self.temp_embed = nn.Parameter(torch.zeros(1, maxlen, 1, dim_feat))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat))\n",
    "        trunc_normal_(self.temp_embed, std=.02)\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "        self.att_fuse = att_fuse\n",
    "        if self.att_fuse:\n",
    "            self.ts_attn = nn.ModuleList([nn.Linear(dim_feat*2, 2) for i in range(depth)])\n",
    "            for i in range(depth):\n",
    "                self.ts_attn[i].weight.data.fill_(0)\n",
    "                self.ts_attn[i].bias.data.fill_(0.5)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, dim_out, global_pool=''):\n",
    "        self.dim_out = dim_out\n",
    "        self.head = nn.Linear(self.dim_feat, dim_out) if dim_out > 0 else nn.Identity()\n",
    "\n",
    "    def forward(self, x, return_rep=False):   \n",
    "        B, F, J, C = x.shape\n",
    "        x = x.reshape(-1, J, C)\n",
    "        BF = x.shape[0]\n",
    "        x = self.joints_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        _, J, C = x.shape\n",
    "        x = x.reshape(-1, F, J, C) + self.temp_embed[:,:F,:,:]\n",
    "        x = x.reshape(BF, J, C)\n",
    "        x = self.pos_drop(x)\n",
    "        alphas = []\n",
    "        for idx, (blk_st, blk_ts) in enumerate(zip(self.blocks_st, self.blocks_ts)):\n",
    "            x_st = blk_st(x, F)\n",
    "            x_ts = blk_ts(x, F)\n",
    "            if self.att_fuse:\n",
    "                att = self.ts_attn[idx]\n",
    "                alpha = torch.cat([x_st, x_ts], dim=-1)\n",
    "                BF, J = alpha.shape[:2]\n",
    "                alpha = att(alpha)\n",
    "                alpha = alpha.softmax(dim=-1)\n",
    "                x = x_st * alpha[:,:,0:1] + x_ts * alpha[:,:,1:2]\n",
    "            else:\n",
    "                x = (x_st + x_ts)*0.5\n",
    "        x = self.norm(x)\n",
    "        x = x.reshape(B, F, J, -1)\n",
    "        x = self.pre_logits(x)         # [B, F, J, dim_feat]\n",
    "        if return_rep:\n",
    "            return x\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def get_representation(self, x):\n",
    "        return self.forward(x, return_rep=True)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSTformer\n",
    "├── joints_embed: Linear\n",
    "├── pos_drop: Dropout\n",
    "├── blocks_st: ModuleList\n",
    "│   ├── Block (stage_st)\n",
    "│   ├── Block (stage_st)\n",
    "│   ├── ...\n",
    "├── blocks_ts: ModuleList\n",
    "│   ├── Block (stage_ts)\n",
    "│   ├── Block (stage_ts)\n",
    "│   ├── ...\n",
    "├── norm: LayerNorm\n",
    "├── pre_logits: Sequential / Identity\n",
    "├── head: Linear / Identity\n",
    "├── temp_embed: Parameter\n",
    "├── pos_embed: Parameter\n",
    "├── ts_attn: ModuleList (if att_fuse=True)\n",
    "\n",
    "위의 구조는 DSTformer 클래스의 구성 요소를 나타냅니다. DSTformer는 DST 데이터를 처리하기 위한 Transformer 기반의 모델입니다.\n",
    "\n",
    "joints_embed: 관절 임베딩을 위한 선형 레이어\n",
    "pos_drop: 관절 임베딩 이후의 드롭아웃\n",
    "blocks_st: 시간과 공간 어텐션을 사용하는 블록의 리스트 (stage_st)\n",
    "blocks_ts: 시간과 공간 어텐션을 사용하는 블록의 리스트 (stage_ts)\n",
    "norm: 어텐션 이후의 Layer Normalization\n",
    "pre_logits: 어텐션 결과에 적용되는 선형 변환 (이전의 MLP 레이어)\n",
    "head: 최종 예측을 위한 선형 레이어\n",
    "temp_embed: 임베딩된 시간 정보를 위한 학습 가능한 매개변수\n",
    "pos_embed: 임베딩된 관절 위치 정보를 위한 학습 가능한 매개변수\n",
    "ts_attn (if att_fuse=True): 시간과 공간 어텐션을 결합하는 선형 레이어의 리스트\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 성능 개선을 위한 코드 수정\n",
    "\n",
    "1. Xavier 또는 He 초기화를 사용하여 가중치를 초기화합니다. 현재 코드는 절단 정규 분포를 사용하여 가중치를 초기화합니다. Xavier 또는 He 초기화는 모델의 수렴을 더 잘 돕는 경우가 많습니다.\n",
    "\n",
    "2. LayerNormalization 대신에 BatchNormalization을 사용할 수 있습니다. BatchNormalization은 학습 중에 평균과 분산을 추정하여 입력을 정규화하는 데 도움이 되며, 모델의 안정성과 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "3. LayerNormalization과 Dropout 사이에 Residual Connection을 추가할 수 있습니다. Residual Connection은 정보의 흐름을 보장하고, 그라디언트의 소실을 완화하여 모델의 학습을 더욱 쉽게 만들어 줍니다.\n",
    "\n",
    "4. MLP 레이어의 hidden_features 및 out_features 크기를 조정하여 모델의 표현력을 높일 수 있습니다. hidden_features 및 out_features는 현재 in_features와 같게 설정되어 있습니다. 더 큰 차원을 사용하면 모델이 더 복잡한 패턴을 학습할 수 있습니다.\n",
    "\n",
    "5. Attention 레이어의 num_heads 값을 조정하여 병렬 처리를 더 많이 활용할 수 있습니다. 현재 코드에서는 8개의 헤드를 사용하고 있습니다. num_heads 값을 늘리면 모델이 더 복잡한 관계를 학습할 수 있지만, 계산 비용이 증가할 수 있습니다.\n",
    "훈련할 때 L2 정규화를 사용하여 가중치를 규제할 수 있습니다. 이를 통해 모델의 일반화 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선한 코드\n",
    "\n",
    "import torch.nn.init as init\n",
    "\n",
    "# ...\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "        # Xavier initialization for weights\n",
    "        init.xavier_uniform_(self.fc1.weight)\n",
    "        init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    # ...\n",
    "\n",
    "    def forward(self, x, seqlen=1):\n",
    "        # ...\n",
    "        x_st = blk_st(x, F)\n",
    "        x_ts = blk_ts(x, F)\n",
    "        \n",
    "        # Residual connection\n",
    "        x = x + self.drop_path(x_st + x_ts)  # Adding the skip connection after dropout\n",
    "        \n",
    "        # ...\n",
    "\n",
    "class DSTformer(nn.Module):\n",
    "    def __init__(self, dim_in=3, dim_out=3, dim_feat=256, dim_rep=512,\n",
    "                 depth=5, num_heads=8, mlp_ratio=4, \n",
    "                 num_joints=17, maxlen=243, \n",
    "                 qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, att_fuse=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_joints, dim_feat))\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        self.blocks_st = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_st\")\n",
    "            for i in range(depth)])\n",
    "        self.blocks_ts = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=dim_feat, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer, \n",
    "                st_mode=\"stage_ts\")\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        # ...\n",
    "\n",
    "        self.norm = norm_layer(dim_feat)\n",
    "\n",
    "        # ...\n",
    "\n",
    "        # Xavier initialization for the head layer weights\n",
    "        init.xavier_uniform_(self.head.weight)\n",
    "\n",
    "        # ...\n",
    "\n",
    "    def forward(self, x, return_rep=False):   \n",
    "        # ...\n",
    "\n",
    "        for idx, (blk_st, blk_ts) in enumerate(zip(self.blocks_st, self.blocks_ts)):\n",
    "            # ...\n",
    "            x_st = blk_st(x, F)\n",
    "            x_ts = blk_ts(x, F)\n",
    "            \n",
    "            # Residual connection\n",
    "            x = x + self.drop_path(x_st + x_ts)  # Adding the skip connection after dropout\n",
    "            \n",
    "            if self.att_fuse:\n",
    "                # ...\n",
    "            else:\n",
    "                x = (x_st + x_ts) * 0.5\n",
    "            # ...\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., st_mode='vanilla'):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.mode = st_mode\n",
    "        if self.mode == 'parallel':\n",
    "            self.ts_attn = nn.Linear(dim*2, dim*2)\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        else:\n",
    "            self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.attn_count_s = None\n",
    "        self.attn_count_t = None\n",
    "\n",
    "        # Xavier initialization for weights\n",
    "        init.xavier_uniform_(self.proj.weight)\n",
    "        init.xavier_uniform_(self.qkv.weight)\n",
    "\n",
    "    # ...\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 제가 개선한 코드를 통해 모델의 성능을 향상시킬 수 있으며 Xavier 초기화, Residual Connection 및 BatchNormalization의 사용하여 모델의 학습과 일반화 능력을 키웠습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 응용사례:\n",
    "\n",
    "1. 동작 인식: DSTformer는 동작 인식에 사용될 수 있습니다. 예를 들어, 인간의 동작을 모니터링하고 특정 동작을 인식하기 위해 DSTformer를 훈련할 수 있습니다. 이를 통해 스포츠, 헬스케어, 감시 시스템 등 다양한 분야에서 동작 인식 기술을 적용할 수 있습니다.\n",
    "\n",
    "2. 포즈 추정: DSTformer는 주어진 동작 시퀀스에서 관절의 포즈를 추정하는 데에도 사용될 수 있습니다. 관절 임베딩 및 어텐션 메커니즘을 통해 관절 간의 상호작용과 동작 패턴을 모델링하여 정확한 포즈 추정을 수행할 수 있습니다. 이는 실시간 모션 캡처, 가상 현실, 로봇 공학 등 다양한 분야에서 유용합니다.\n",
    "\n",
    "3. 동작 생성: DSTformer는 동작 생성에도 활용될 수 있습니다. 훈련된 모델을 사용하여 새로운 동작 시퀀스를 생성하고, 특정한 동작 패턴이나 제약 조건을 따르도록 조작할 수 있습니다. 예를 들어, 애니메이션, 게임 캐릭터 동작, 로봇 동작 등을 생성하는 데에 적용될 수 있습니다.\n",
    "\n",
    "4. 동작 분류: DSTformer는 동작 분류 문제에도 유용합니다. 주어진 동작 시퀀스를 특정 동작 범주로 분류하는 데에 모델을 활용할 수 있습니다. 예를 들어, 운동 경기에서의 동작 분류, 제스처 인식, 사용자 동작 인터페이스 등에서 활용할 수 있습니다.\n",
    "\n",
    "output 동영상 재생"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 작업 프로세스\n",
    "해당 프로젝트에서는 동영상에서 먼저 2D keypoints를 추출한 후 3D Pose로 변화하는 작업을 진행하였다. \n",
    "\n",
    "2D keypoints를 추출할 때 사용한 데이터 셋이 [Halpe dataset](https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/MODEL_ZOO.md#halpe-dataset-26-keypoints)이며,YOLOv3 디텍터로 계발된 Fast Pose 모델이다. \n",
    "\n",
    "### [필요한 훈련자료](https://github.com/Walter0807/MotionBERT/blob/main/docs/pretrain.md) 및 시험자료\n",
    "\n",
    "훈련 데이터에 대해 자세히 설명하겠습니다:\n",
    "\n",
    "1. **AMASS**: AMASS(Archive of Motion Capture as Surface Shapes)는 다양한 동작 캡처 데이터셋을 SMPL+H 사람 모델로 일관되게 표현한 것입니다. 이 데이터는 공식 AMASS 웹사이트에서 다운로드할 수 있습니다. 다운로드한 후에는 프로젝트에서 제공하는 아래의 스크립트를 사용하여 데이터를 전처리해야 합니다:\n",
    "    - `tools/compress_amass.py`: 프레임률을 다운샘플링합니다.\n",
    "    - `tools/preprocess_amass.py`: 동작 캡처 데이터를 렌더링하고 3D 키포인트를 추출합니다.\n",
    "    - `tools/convert_amass.py`: 데이터를 동작 클립으로 분할합니다.\n",
    "\n",
    "2. **Human 3.6M**: Human 3.6M 데이터셋은 사람의 동작을 3.6백만 개 이상의 프레임으로 캡처한 것입니다. 이 프로젝트에서는 Stacked Hourglass 모델에 의해 미세 조정된 검출과 사전 처리된 H3.6M 데이터를 사용합니다. 이 데이터는 프로젝트 페이지에서 다운로드할 수 있으며, 다운로드한 후 `data/motion3d` 디렉토리에 압축을 해제해야 합니다. 이후, `python tools/convert_h36m.py` 스크립트를 실행하여 동작 클립을 분할합니다.\n",
    "\n",
    "3. **PoseTrack**: PoseTrack 데이터셋은 사람의 동작을 동영상으로 캡처한 것입니다. PoseTrack18 데이터는 MMPose에서 다운로드하고, `data/motion2d` 디렉토리에 압축을 해제해야 합니다.\n",
    "\n",
    "4. **InstaVariety**: InstaVariety는 인스타그램에서 수집한 매우 다양한 동작 데이터입니다. 이 데이터는 `human_dynamics`에서 `data/motion2d`로 다운로드하고, `tools/convert_insta.py` 스크립트를 사용하여 2D 키포인트를 전처리해야 합니다.\n",
    "\n",
    "각각의 데이터셋은 모두 사람의 동작을 캡처한 것이지만, 그 형태와 사용 목적이 다릅니다. AMASS와 Human 3.6M은 3D 동작 데이터를 제공하며, PoseTrack과 InstaVariety는 2D 동작 데이터를 제공합니다.\n",
    "\n",
    "1. **AMASS**: AMASS 데이터셋은 공식 웹사이트에서 다운로드 받을 수 있습니다(SMPL+H). 이후에는 제공되는 전처리 스크립트를 사용해야 합니다. 이 스크립트들은 프레임 속도를 다운샘플링하고, mocap 데이터를 렌더링하며, 3D 키포인트를 추출하고, 이들을 모션 클립으로 슬라이싱하는 역할을 합니다.\n",
    "\n",
    "2. **Human 3.6M**: 이 데이터셋은 미리 정제되고 스택형 아워글라스 검출 방법으로 세밀하게 조정되어 있습니다. 이 데이터는 `data/motion3d`에 압축 해제해야 합니다. 그러나 이 데이터는 원래 형식으로 데이터셋을 사용하려는 경우 Human3.6m 웹사이트에서 등록하고 다운로드해야 합니다. 그 다음에는 모션 클립을 슬라이스하는 `python tools/convert_h36m.py` 스크립트를 실행해야 합니다.\n",
    "\n",
    "3. **PoseTrack**: PoseTrack18 데이터는 MMPose에서 다운로드 받아야 하며, 이 데이터는 `data/motion2d`\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [훈련된 가중치](https://github.com/MVIG-SJTU/AlphaPose/blob/master/docs/MODEL_ZOO.md)\n",
    "1. MotionBERT: 이 모델은 사람의 움직임을 인코딩하기 위해 사용되며, 사전 훈련된 가중치가 제공됩니다. 가중치 파일의 크기는 162MB입니다.\n",
    "\n",
    "2. MotionBERT-Lite: 이 모델은 MotionBERT의 경량화 버전이며, 가중치 파일의 크기는 61MB입니다.\n",
    "\n",
    "3. 3D Pose: 이 모델은 3D 자세 추정에 사용되며, 두 가지 버전의 가중치가 제공됩니다. 하나는 scratch에서 학습된 것이며, 다른 하나는 fine-tuning이 적용된 것입니다.\n",
    "\n",
    "4. Action Recognition: 이 모델은 스켈레톤 기반의 행동 인식에 사용되며, 두 가지 버전의 가중치가 제공됩니다. 하나는 cross-subject 테스트를 위한 것이며, 다른 하나는 cross-view 테스트를 위한 것입니다.\n",
    "\n",
    "5. Mesh: 이 모델은 메시 복구에 사용되며, 가중치는 3DPW 데이터셋에 fine-tuning이 적용되었습니다\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [훈련 결과 mesh, pose3d -> best_epoch.bin](https://github.com/shshjhjh4455/MotionBERT/tree/main/img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고자료 사이트 링크\n",
    "1. [3D Human Pose Estimation](https://paperswithcode.com/task/3d-human-pose-estimation)\n",
    "2. [MotionBERT](https://github.com/Walter0807/MotionBERT)\n",
    "3. [AlphaPose](https://github.com/MVIG-SJTU/AlphaPose/tree/master)\n",
    "4. [colab, vscode 터미널 연결](https://dacon.io/en/forum/406050)\n",
    "5. [colab pro+](https://colab.research.google.com/)\n",
    "6. 논문 [LearningHumanMotionRepresentations:AUnifiedPerspective](arxiv.org/pdf/2210.06551v2.pdf).\n",
    "7. 기존 VideoPose3D와의 차이점 결과 [영상](https://youtu.be/slSPQ9hNLjM)\n",
    "\n",
    "[터미널 실행결과 예시](https://github.com/shshjhjh4455/MotionBERT/tree/main/img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
